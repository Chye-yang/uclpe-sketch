{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🥇 Main Experiments 🥈🥉\n",
    "\n",
    "🕶️ **To evaluate the frequency estimation performance, we compare our UCL-sketch with six baselines on four datasets.**\n",
    "\n",
    "- **Baselines** : `CM-sketch`, `C-sketch`, `Elastic Sketch`, `UnivMon`, `Nitrosketch`, `Learned CM-sketch`, and `Learned C-sketch`.\n",
    "\n",
    "- **Datasets** : *CAIDA*, *Retail*, *Kosarak*, and *Zipfian*\n",
    "\n",
    "- **Metrics** : *Average Absolute Error (AAE)*, *Average Relative Error (ARE)*, *Weighted Mean Relative Difference (WMRD)*, and *Entropy Absolute Error*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚄  Step 1 - Import necessary packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from load_data import readTraces\n",
    "from Sketching.unimon import UnivMon\n",
    "from Sketching.cs_sketch import Csketch\n",
    "from Sketching.cm_sketch import CMsketch\n",
    "from Utils.training import learningSolver\n",
    "from UCL_sketch.ucl_sketch import UCLSketch\n",
    "from Sketching.nitro_sketch import NitroSketch\n",
    "from Sketching.elastic_sketch import ElasticSketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔨 Step 2 - Configure your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Main Experiment')\n",
    "\n",
    "parser.add_argument('--data', type=str, default='network', help='data',\n",
    "                     choices=['retail', 'kosarak', 'network', 'synthetic'])\n",
    "parser.add_argument('--root_path', type=str, default='./data/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='test-8s.dat', help='data file',\n",
    "                    choices=['retail.dat', 'kosarak.dat', 'test-8s.dat', ''])\n",
    "parser.add_argument('--seed', type=int, default=12345, help='random seed')\n",
    "\n",
    "parser.add_argument('--slot_num', type=int, default=1200, help='Number of slots in the hash table')\n",
    "parser.add_argument('--bf_width', type=int, default=390000, help='Length of the Bloom Filter')\n",
    "parser.add_argument('--bf_hash', type=int, default=8, help='Number of used hash functions of the Bloom Filter')\n",
    "parser.add_argument('--key_size', type=int, default=13, help='KEY_T_SIZE')\n",
    "parser.add_argument('--hash_num', type=int, default=4, help='Number of used hash functions of the Sketch')\n",
    "parser.add_argument('--bucket_dim', type=int, default=512, help='Number of dimensions of each hash vector in the Sketch')\n",
    "parser.add_argument('--d_model', type=int, default=128, help='dimension of hidden states (d_model)')\n",
    "parser.add_argument('--break_number', type=int, default=1000000, help='number of stream data')\n",
    "parser.add_argument('--e_layers', type=int, default=0, help='Number of middle layers')\n",
    "parser.add_argument('--dropout', type=float, default=0., help='dropout')\n",
    "parser.add_argument('--d_share', type=int, default=1024, help='length of the logical bucket')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints', help='location to store model checkpoints')\n",
    "\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--train_epochs', type=int, default=300, help='train epochs')\n",
    "parser.add_argument('--patience', type=int, default=30, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3, help='optimizer initial learning rate')\n",
    "parser.add_argument('--lradj', type=str, default='type1',help='adjust learning rate')\n",
    "\n",
    "parser.add_argument('--save_pred', action='store_true', help='whether to save the estimated frequency', default=False)\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3',help='device ids of multile gpus')\n",
    "\n",
    "parser.add_argument('--interval', type=int, default=1000, help='sampling inserval')\n",
    "parser.add_argument('--num_samples', type=int, default=128, help='maintained samples (sliding window)')\n",
    "parser.add_argument('--ablation', type=int, default=0, help='ablational type')\n",
    "\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "args.use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "    print(args.gpu)\n",
    "\n",
    "print(f\"Global seed set to {args.seed}\")\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Memory Usage -----\n",
      "Hash Table Size(Byte): 25221 (24.63 KB)\n",
      "Bloom Filter Size(Byte): 48751 (47.61 KB)\n",
      "CM Sketch Size(Byte): 16672 (16.28 KB)\n",
      "Total Memory(MB): 88.52 KB\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "ucl_sketch = UCLSketch(args.slot_num, args.bucket_dim, args.hash_num, args.bf_width, args.bf_hash, args.key_size)\n",
    "_ = ucl_sketch.get_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM-Sketch size: 160.84 KB\n",
      "C-Sketch size: 160.84 KB\n",
      "Nitro-Sketch size: 160.88 KB\n",
      "Elastic Sketch size: 127.24 KB\n",
      "UnivMon size: 192.25 KB\n"
     ]
    }
   ],
   "source": [
    "bucket_dim = 1024\n",
    "slot_num = 3500\n",
    "\n",
    "c_sketch = Csketch(bucket_dim*5, 4, args.key_size)\n",
    "lc_sketch = Csketch(bucket_dim*3, 4, args.key_size)\n",
    "cm_sketch = CMsketch(bucket_dim*5, 4, args.key_size)\n",
    "lcm_sketch = CMsketch(bucket_dim*3, 4, args.key_size)\n",
    "nitro_sketch = NitroSketch(bucket_dim*5, 4, args.key_size)\n",
    "elastic_sketch = ElasticSketch(slot_num//65, slot_num//100, bucket_dim*3, 4, args.key_size)\n",
    "univmon = UnivMon(2, bucket_dim*4, 4, args.key_size)\n",
    "\n",
    "ls = Csketch(bucket_dim*3, 4, args.key_size)\n",
    "\n",
    "print(f'CM-Sketch size: {(c_sketch.get_memory_usage() / (1024)):.2f} KB')\n",
    "print(f'C-Sketch size: {(cm_sketch.get_memory_usage() / (1024)):.2f} KB')\n",
    "print(f'Nitro-Sketch size: {(nitro_sketch.get_memory_usage() / (1024)):.2f} KB')\n",
    "print(f'Elastic Sketch size: {(elastic_sketch.get_memory_usage() / (1024)):.2f} KB')\n",
    "print(f'UnivMon size: {(univmon.get_memory_usage() / (1024)):.2f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⏩ Step 3 - Insert data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in packets data...\n",
      "Successfully read in 3670738 items.\n"
     ]
    }
   ],
   "source": [
    "path = args.root_path + args.data_path\n",
    "size, traces = readTraces(path, args.data, args.key_size)\n",
    "\n",
    "if size > args.break_number:\n",
    "    traces = traces[:args.break_number]\n",
    "    size = args.break_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting packets into the sketch: 100%|██████████| 1000000/1000000 [00:45<00:00, 22120.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth已写入到 ground_truth.txt\n",
      "ground_truth_1.txt 已写入到 ground_truth_files\n",
      "ground_truth_2.txt 已写入到 ground_truth_files\n",
      "ground_truth_3.txt 已写入到 ground_truth_files\n",
      "ground_truth_4.txt 已写入到 ground_truth_files\n",
      "ground_truth_5.txt 已写入到 ground_truth_files\n",
      "ground_truth_6.txt 已写入到 ground_truth_files\n",
      "ground_truth_7.txt 已写入到 ground_truth_files\n",
      "ground_truth_8.txt 已写入到 ground_truth_files\n",
      "ground_truth_9.txt 已写入到 ground_truth_files\n",
      "ground_truth_10.txt 已写入到 ground_truth_files\n",
      "ground_truth_11.txt 已写入到 ground_truth_files\n",
      "ground_truth_12.txt 已写入到 ground_truth_files\n",
      "ground_truth_13.txt 已写入到 ground_truth_files\n",
      "ground_truth_14.txt 已写入到 ground_truth_files\n",
      "ground_truth_15.txt 已写入到 ground_truth_files\n",
      "ground_truth_16.txt 已写入到 ground_truth_files\n",
      "ground_truth_17.txt 已写入到 ground_truth_files\n",
      "ground_truth_18.txt 已写入到 ground_truth_files\n",
      "ground_truth_19.txt 已写入到 ground_truth_files\n",
      "所有 ground_truth 数据已分批写入到 ground_truth_files 目录下的 19 个文件中。\n",
      "Insert 1000000 items with 93703 distinct keys. Meanwhile, 129 points is sampled.\n"
     ]
    }
   ],
   "source": [
    "packetCnt = 0\n",
    "ground_truth = {}\n",
    "sample_initial = size - (args.interval + 12) * args.num_samples\n",
    "\n",
    "args.bucket_dim = ucl_sketch.cm.width\n",
    "samples = np.empty([0, args.hash_num, args.bucket_dim])\n",
    "\n",
    "with tqdm(initial=0, total=size, desc='Inserting packets into the sketch') as pbar:\n",
    "    for idx, trace in enumerate(traces):\n",
    "        if trace in ground_truth:\n",
    "            ground_truth[trace] += 1\n",
    "        else:\n",
    "            ground_truth[trace] = 1\n",
    "            \n",
    "        \n",
    "        ucl_sketch.insert(trace)\n",
    "        packetCnt += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if idx > sample_initial and idx % args.interval == 0:\n",
    "            sample = ucl_sketch.get_current_state(return_A=False)\n",
    "            samples = np.row_stack([samples, sample])\n",
    "\n",
    "gt_path = 'ground_truth.txt'\n",
    "with open(gt_path, 'w', encoding='utf-8') as f:\n",
    "    for key, value in ground_truth.items():\n",
    "        f.write(f'{key}\\t{value}\\n')\n",
    "print(f'ground_truth已写入到 {gt_path}')\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "output_dir = 'ground_truth_files'\n",
    "os.makedirs(output_dir, exist_ok=True) # 创建输出目录\n",
    "\n",
    "batch_size = 5000\n",
    "file_count = 0\n",
    "current_batch_data = {}\n",
    "\n",
    "for key, value in ground_truth.items():\n",
    "    current_batch_data[key] = value\n",
    "    if len(current_batch_data) >= batch_size:\n",
    "        file_count += 1\n",
    "        gt_path = os.path.join(output_dir, f'ground_truth_{file_count}.txt')\n",
    "        with open(gt_path, 'w', encoding='utf-8') as f:\n",
    "            for k, v in current_batch_data.items():\n",
    "                f.write(f'{k}\\t{v}\\n')\n",
    "        print(f'ground_truth_{file_count}.txt 已写入到 {output_dir}')\n",
    "        current_batch_data = {} # 清空当前批次数据\n",
    "\n",
    "# 处理最后一个可能不满 batch_size 的批次\n",
    "if current_batch_data:\n",
    "    file_count += 1\n",
    "    gt_path = os.path.join(output_dir, f'ground_truth_{file_count}.txt')\n",
    "    with open(gt_path, 'w', encoding='utf-8') as f:\n",
    "        for k, v in current_batch_data.items():\n",
    "            f.write(f'{k}\\t{v}\\n')\n",
    "    print(f'ground_truth_{file_count}.txt 已写入到 {output_dir}')\n",
    "\n",
    "print(f'所有 ground_truth 数据已分批写入到 {output_dir} 目录下的 {file_count} 个文件中。')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GT_sorted = sorted(ground_truth, key=lambda x: ground_truth[x])\n",
    "topk_keys = GT_sorted[- args.slot_num:]\n",
    "GT_ideal = GT_sorted[:- args.slot_num]\n",
    "\n",
    "for key in GT_ideal:\n",
    "    value = ground_truth[key]\n",
    "    lc_sketch.insert(key, value)\n",
    "    lcm_sketch.insert(key, value)\n",
    "    c_sketch.insert(key, value)\n",
    "    cm_sketch.insert(key, value)\n",
    "    nitro_sketch.insert(key, value)\n",
    "    elastic_sketch.insert(key, value)\n",
    "    univmon.insert(key, value)\n",
    "\n",
    "for key in topk_keys:\n",
    "    value = ground_truth[key]\n",
    "    c_sketch.insert(key, value)\n",
    "    cm_sketch.insert(key, value)\n",
    "    nitro_sketch.insert(key, value)\n",
    "    elastic_sketch.insert(key, value)\n",
    "    univmon.insert(key, value)\n",
    "\n",
    "print(f'Insert {packetCnt} items with {len(ground_truth)} distinct keys. Meanwhile, {samples.shape[0]} points is sampled.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⛺ Step 4 - Train the learning-based solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, index = ucl_sketch.get_current_state(return_A=True)\n",
    "solver = learningSolver(args, A.shape[1])\n",
    "solver.train(samples, A, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Step 5 - Per-key frequency query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'GT': [], 'UCL': [], 'ES': [], 'UM': [], 'CM': [],\n",
    "           'CS': [], 'NS': [], 'LCM': [], 'LCS': []}\n",
    "\n",
    "print('Querying for all keys ...')\n",
    "\n",
    "for key, value in ground_truth.items():\n",
    "    c_ans = c_sketch.query(key)\n",
    "    cm_ans = cm_sketch.query(key)\n",
    "    nitro_ans = nitro_sketch.query(key)\n",
    "    ela_ans = elastic_sketch.query(key)\n",
    "    uni_ans = univmon.query(key)\n",
    "\n",
    "    if key in topk_keys:\n",
    "        lc_ans = value\n",
    "        lcm_ans = value\n",
    "    else:\n",
    "        lc_ans = lc_sketch.query(key)\n",
    "        lcm_ans = lcm_sketch.query(key)\n",
    "\n",
    "    if ucl_sketch.cmResult == {}:\n",
    "        test_sample = ucl_sketch.get_current_state(return_A=False)\n",
    "        x = solver.test(test_sample)\n",
    "        x = np.ceil(x.squeeze()).astype(np.int32)\n",
    "\n",
    "    ucl_ans = ucl_sketch.query(key, x)\n",
    "\n",
    "    results['GT'].append(value)\n",
    "    results['UCL'].append(ucl_ans)\n",
    "    results['ES'].append(ela_ans)\n",
    "    results['UM'].append(uni_ans)\n",
    "    results['CM'].append(cm_ans)\n",
    "    results['CS'].append(c_ans)\n",
    "    results['NS'].append(nitro_ans)\n",
    "    results['LCM'].append(lcm_ans)\n",
    "    results['LCS'].append(lc_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 Step 6 - Caculate related metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.mertrics import *\n",
    "\n",
    "GT = results['GT']\n",
    "\n",
    "AAE_R = []\n",
    "ARE_R = []\n",
    "WMRD_R = []\n",
    "EAE_R = []\n",
    "\n",
    "print('Performace of UCL-sketch:')\n",
    "ET = results['UCL']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)\n",
    "\n",
    "print('Performace of CM-sketch:')\n",
    "ET = results['CM']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)\n",
    "\n",
    "print('Performace of C-sketch:')\n",
    "ET = results['CS']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)\n",
    "\n",
    "print('Performace of Ideally Learned CM-sketch:')\n",
    "ET = results['LCM']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)\n",
    "\n",
    "print('Performace of Ideally Learned C-sketch:')\n",
    "ET = results['LCS']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)\n",
    "\n",
    "print('Performace of Elastic Sketch:')\n",
    "ET = results['ES']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)\n",
    "\n",
    "print('Performace of UnivMon:')\n",
    "ET = results['UM']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)\n",
    "\n",
    "print('Performace of NitroSketch:')\n",
    "ET = results['NS']\n",
    "AAE = average_absolute_error(GT, ET)\n",
    "ARE = average_relative_error(GT, ET)\n",
    "WMRD = weighted_mean_relative_difference(GT, ET)\n",
    "EAE = entropy_absolute_error(GT, ET)\n",
    "print(f'—— AAE: {AAE:.4f}, ARE: {ARE:.4f}, WMRD: {WMRD:.4f}, and Entropy Absolute Error: {EAE:.4f}')\n",
    "AWE = average_weighted_error(GT, ET)\n",
    "\n",
    "AAE_R.append(int(AAE*100)/100)\n",
    "ARE_R.append(int(ARE*100)/100)\n",
    "WMRD_R.append(int(WMRD*100)/100)\n",
    "EAE_R.append(int(EAE*100)/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📶 Step 7 - Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(15, 9))\n",
    "\n",
    "width = 0.5\n",
    "x_n = ['UCL', 'CM', 'CS', 'LCM', 'LCS', 'ES', 'UM', 'NS']\n",
    "x = np.arange(len(x_n))\n",
    "\n",
    "ax1.bar(x, AAE_R, width=width, edgecolor='k')\n",
    "ax1.set_xticks(x, labels=x_n, fontproperties='Times New Roman', fontsize=12)\n",
    "ax1.set_ylabel('AAE', fontdict={'family': 'Times New Roman', 'size': 18})\n",
    "ax1.tick_params('both', labelsize=12)\n",
    "ax1.grid(axis='y', linestyle='-.', lw=0.75)\n",
    "\n",
    "ax2.bar(x, ARE_R, width=width, edgecolor='k')\n",
    "ax2.set_xticks(x, labels=x_n, fontproperties='Times New Roman', fontsize=12)\n",
    "ax2.set_ylabel('ARE', fontdict={'family': 'Times New Roman', 'size': 18})\n",
    "ax2.tick_params('both', labelsize=12)\n",
    "ax2.grid(axis='y', linestyle='-.', lw=0.75)\n",
    "\n",
    "ax3.bar(x, WMRD_R, width=width, edgecolor='k')\n",
    "ax3.set_xticks(x, labels=x_n, fontproperties='Times New Roman', fontsize=12)\n",
    "ax3.set_ylabel('WMRD', fontdict={'family': 'Times New Roman', 'size': 18})\n",
    "ax3.tick_params('both', labelsize=12)\n",
    "ax3.grid(axis='y', linestyle='-.', lw=0.5)\n",
    "\n",
    "ax4.bar(x, EAE_R, width=width, edgecolor='k')\n",
    "ax4.set_xticks(x, labels=x_n, fontproperties='Times New Roman', fontsize=12)\n",
    "ax4.set_ylabel('Entropy Absolute Error', fontdict={'family': 'Times New Roman', 'size': 18})\n",
    "ax4.tick_params('both', labelsize=12)\n",
    "ax4.grid(axis='y', linestyle='-.', lw=0.5)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
